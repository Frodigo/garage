{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text preprocessing experiments\n",
    "\n",
    "My goal is to find a solution for text preprocessing that allows to remove unnecessary, not important words from text before sending it to LLM.\n",
    "\n",
    "## Setup libs"
   ],
   "id": "c31db1b30e7a4bf5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:19.658578Z",
     "start_time": "2025-07-07T20:54:19.192531Z"
    }
   },
   "source": "%pip install trafilatura tiktoken",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trafilatura in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (2.0.0)\r\n",
      "Requirement already satisfied: tiktoken in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: certifi in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (2025.6.15)\r\n",
      "Requirement already satisfied: charset_normalizer>=3.4.0 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (3.4.2)\r\n",
      "Requirement already satisfied: courlan>=1.3.2 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (1.3.2)\r\n",
      "Requirement already satisfied: htmldate>=1.9.2 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (1.9.3)\r\n",
      "Requirement already satisfied: justext>=3.0.1 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (3.0.2)\r\n",
      "Requirement already satisfied: lxml>=5.3.0 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (5.4.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from trafilatura) (2.5.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from tiktoken) (2024.11.6)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from tiktoken) (2.32.4)\r\n",
      "Requirement already satisfied: babel>=2.16.0 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from courlan>=1.3.2->trafilatura) (2.16.0)\r\n",
      "Requirement already satisfied: tld>=0.13 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from courlan>=1.3.2->trafilatura) (0.13.1)\r\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from htmldate>=1.9.2->trafilatura) (1.2.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2024.2 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\r\n",
      "Requirement already satisfied: tzlocal>=0.2 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (4.3.1)\r\n",
      "Requirement already satisfied: lxml_html_clean in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.7)\r\n",
      "Requirement already satisfied: pytz-deprecation-shim in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from tzlocal>=0.2->dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (0.1.0.post0)\r\n",
      "Requirement already satisfied: tzdata in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (from pytz-deprecation-shim->tzlocal>=0.2->dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sample texts\n",
    "\n",
    "In this experiment, I'm going to use a few recent newsletters from JS Weekly. The last one is a number 743. Let's fetch recent 10 issues\n",
    "\n",
    "### Download texts"
   ],
   "id": "fb4d9cc85236e297"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:23.505160Z",
     "start_time": "2025-07-07T20:54:19.663048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trafilatura import fetch_url, extract\n",
    "\n",
    "latest_js_weekly_issue = 743\n",
    "js_weekly_base_url = \"https://javascriptweekly.com/issues/\"\n",
    "downloaded_issues = []\n",
    "\n",
    "for issue_number in range(latest_js_weekly_issue, latest_js_weekly_issue - 11, -1 ):\n",
    "    issue = fetch_url(js_weekly_base_url + str(issue_number))\n",
    "    issue = extract(issue)\n",
    "    downloaded_issues.append({\n",
    "        'issue_number': issue_number,\n",
    "        'content': issue,\n",
    "    })\n"
   ],
   "id": "f9e4f8a920de08f",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenize downloaded texts\n",
    "\n",
    "Let's see how many tokens have each text."
   ],
   "id": "366af6eff74cf4a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:23.518484Z",
     "start_time": "2025-07-07T20:54:23.513245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "all_issues_tokens = 0\n",
    "\n",
    "for issue in downloaded_issues:\n",
    "    tokens = len(tokenizer.encode(issue['content']))\n",
    "    issue['tokens_numbers'] = tokens\n",
    "    all_issues_tokens += tokens\n",
    "    print(f\"issue_number: {issue['issue_number']}, tokens: {tokens}\")\n",
    "\n",
    "print(f\"all_issues_tokens: {all_issues_tokens}\")"
   ],
   "id": "65c1ff2a31d82132",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue_number: 743, tokens: 1007\n",
      "issue_number: 742, tokens: 1099\n",
      "issue_number: 741, tokens: 801\n",
      "issue_number: 740, tokens: 914\n",
      "issue_number: 739, tokens: 863\n",
      "issue_number: 738, tokens: 1075\n",
      "issue_number: 737, tokens: 951\n",
      "issue_number: 736, tokens: 944\n",
      "issue_number: 735, tokens: 1066\n",
      "issue_number: 734, tokens: 994\n",
      "issue_number: 733, tokens: 1090\n",
      "all_issues_tokens: 10804\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing function\n",
    "\n",
    "Let's implement a function that will be used for experimenting\n"
   ],
   "id": "d4165382fb57277a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:23.572499Z",
     "start_time": "2025-07-07T20:54:23.569861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_issues_content(processing_function, function_name=\"processing function\"):\n",
    "    print(f\"all_issues_tokens before this step: {all_issues_tokens}\")\n",
    "    new_all_issues_tokens = 0\n",
    "    for issue in downloaded_issues:\n",
    "        print(f\"tokens before {function_name}: {issue['tokens_numbers']}\")\n",
    "        issue['content'] = processing_function(issue['content'])\n",
    "        issue['tokens_numbers'] = len(tokenizer.encode(issue['content']))\n",
    "        print(f\"tokens after {function_name}: {issue['tokens_numbers']}\")\n",
    "        new_all_issues_tokens += issue['tokens_numbers']\n",
    "\n",
    "    print(f\"all_issues_tokens after this step: {new_all_issues_tokens}\")\n",
    "    return new_all_issues_tokens"
   ],
   "id": "a6128c1336d015d8",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove emojis\n",
   "id": "c041e2eb9d838195"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.043773Z",
     "start_time": "2025-07-07T20:54:23.584152Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install emoji demoji",
   "id": "6f0aa43b4601eb72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (2.14.1)\r\n",
      "Requirement already satisfied: demoji in /home/frodigo/anaconda3/envs/nitrodigest/lib/python3.11/site-packages (1.1.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.055320Z",
     "start_time": "2025-07-07T20:54:24.053819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# function for removing emojis\n",
    "\n",
    "import demoji\n",
    "import emoji\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = demoji.replace(text, '')\n",
    "\n",
    "    return text"
   ],
   "id": "93baa548334aa50b",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.227356Z",
     "start_time": "2025-07-07T20:54:24.096727Z"
    }
   },
   "cell_type": "code",
   "source": "all_issues_tokens = process_issues_content(remove_emojis, \"removing emojis\")",
   "id": "3ff5056b8578ee0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_issues_tokens before this step: 10804\n",
      "tokens before removing emojis: 1007\n",
      "tokens after removing emojis: 967\n",
      "tokens before removing emojis: 1099\n",
      "tokens after removing emojis: 1074\n",
      "tokens before removing emojis: 801\n",
      "tokens after removing emojis: 778\n",
      "tokens before removing emojis: 914\n",
      "tokens after removing emojis: 886\n",
      "tokens before removing emojis: 863\n",
      "tokens after removing emojis: 829\n",
      "tokens before removing emojis: 1075\n",
      "tokens after removing emojis: 1050\n",
      "tokens before removing emojis: 951\n",
      "tokens after removing emojis: 928\n",
      "tokens before removing emojis: 944\n",
      "tokens after removing emojis: 924\n",
      "tokens before removing emojis: 1066\n",
      "tokens after removing emojis: 1043\n",
      "tokens before removing emojis: 994\n",
      "tokens after removing emojis: 973\n",
      "tokens before removing emojis: 1090\n",
      "tokens after removing emojis: 1063\n",
      "all_issues_tokens after this step: 10515\n"
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove email addresses",
   "id": "7a58dda52d016378"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.237035Z",
     "start_time": "2025-07-07T20:54:24.235667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def remove_email_addresses(text: str) -> str:\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    return email_pattern.sub('', text)"
   ],
   "id": "ec33a6ff7788376e",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.285228Z",
     "start_time": "2025-07-07T20:54:24.280841Z"
    }
   },
   "cell_type": "code",
   "source": "all_issues_tokens = process_issues_content(remove_email_addresses, \"removing email addresses\")",
   "id": "e48f852472dbf501",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_issues_tokens before this step: 10515\n",
      "tokens before removing email addresses: 967\n",
      "tokens after removing email addresses: 967\n",
      "tokens before removing email addresses: 1074\n",
      "tokens after removing email addresses: 1074\n",
      "tokens before removing email addresses: 778\n",
      "tokens after removing email addresses: 778\n",
      "tokens before removing email addresses: 886\n",
      "tokens after removing email addresses: 886\n",
      "tokens before removing email addresses: 829\n",
      "tokens after removing email addresses: 829\n",
      "tokens before removing email addresses: 1050\n",
      "tokens after removing email addresses: 1050\n",
      "tokens before removing email addresses: 928\n",
      "tokens after removing email addresses: 928\n",
      "tokens before removing email addresses: 924\n",
      "tokens after removing email addresses: 924\n",
      "tokens before removing email addresses: 1043\n",
      "tokens after removing email addresses: 1043\n",
      "tokens before removing email addresses: 973\n",
      "tokens after removing email addresses: 973\n",
      "tokens before removing email addresses: 1063\n",
      "tokens after removing email addresses: 1063\n",
      "all_issues_tokens after this step: 10515\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove phone numbers\n",
   "id": "5a4c5e41bd4548a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.337150Z",
     "start_time": "2025-07-07T20:54:24.335606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_phone_numbers(text: str) -> str:\n",
    "    phone_pattern = re.compile(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b')\n",
    "    return phone_pattern.sub('', text)"
   ],
   "id": "4e7fe415de5e4180",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.358636Z",
     "start_time": "2025-07-07T20:54:24.354450Z"
    }
   },
   "cell_type": "code",
   "source": "all_issues_tokens = process_issues_content(remove_phone_numbers, \"removing phone numbers\")",
   "id": "20c4bcf33cc8d6dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_issues_tokens before this step: 10515\n",
      "tokens before removing phone numbers: 967\n",
      "tokens after removing phone numbers: 967\n",
      "tokens before removing phone numbers: 1074\n",
      "tokens after removing phone numbers: 1074\n",
      "tokens before removing phone numbers: 778\n",
      "tokens after removing phone numbers: 778\n",
      "tokens before removing phone numbers: 886\n",
      "tokens after removing phone numbers: 886\n",
      "tokens before removing phone numbers: 829\n",
      "tokens after removing phone numbers: 829\n",
      "tokens before removing phone numbers: 1050\n",
      "tokens after removing phone numbers: 1050\n",
      "tokens before removing phone numbers: 928\n",
      "tokens after removing phone numbers: 928\n",
      "tokens before removing phone numbers: 924\n",
      "tokens after removing phone numbers: 924\n",
      "tokens before removing phone numbers: 1043\n",
      "tokens after removing phone numbers: 1043\n",
      "tokens before removing phone numbers: 973\n",
      "tokens after removing phone numbers: 973\n",
      "tokens before removing phone numbers: 1063\n",
      "tokens after removing phone numbers: 1063\n",
      "all_issues_tokens after this step: 10515\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Remove urls",
   "id": "9d2403f8a4c7c7c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.405744Z",
     "start_time": "2025-07-07T20:54:24.403988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_urls(text: str) -> str:\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)"
   ],
   "id": "77bcb07e4391da21",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.455755Z",
     "start_time": "2025-07-07T20:54:24.449929Z"
    }
   },
   "cell_type": "code",
   "source": "all_issues_tokens = process_issues_content(remove_urls, \"removing urls\")",
   "id": "8292aa5a14d51e2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_issues_tokens before this step: 10515\n",
      "tokens before removing urls: 967\n",
      "tokens after removing urls: 967\n",
      "tokens before removing urls: 1074\n",
      "tokens after removing urls: 1074\n",
      "tokens before removing urls: 778\n",
      "tokens after removing urls: 778\n",
      "tokens before removing urls: 886\n",
      "tokens after removing urls: 886\n",
      "tokens before removing urls: 829\n",
      "tokens after removing urls: 829\n",
      "tokens before removing urls: 1050\n",
      "tokens after removing urls: 1050\n",
      "tokens before removing urls: 928\n",
      "tokens after removing urls: 928\n",
      "tokens before removing urls: 924\n",
      "tokens after removing urls: 924\n",
      "tokens before removing urls: 1043\n",
      "tokens after removing urls: 1043\n",
      "tokens before removing urls: 973\n",
      "tokens after removing urls: 973\n",
      "tokens before removing urls: 1063\n",
      "tokens after removing urls: 1063\n",
      "all_issues_tokens after this step: 10515\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Remove special chars",
   "id": "906586ffc13277c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.509670Z",
     "start_time": "2025-07-07T20:54:24.506573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_special_chars(text: str) -> str:\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s.,!?;:()\\'\\-]', '', text)"
   ],
   "id": "ba470c13e8397deb",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.570037Z",
     "start_time": "2025-07-07T20:54:24.564643Z"
    }
   },
   "cell_type": "code",
   "source": "all_issues_tokens = process_issues_content(remove_special_chars, \"removing special chars\")",
   "id": "5399647338ffea94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_issues_tokens before this step: 10515\n",
      "tokens before removing special chars: 967\n",
      "tokens after removing special chars: 925\n",
      "tokens before removing special chars: 1074\n",
      "tokens after removing special chars: 1038\n",
      "tokens before removing special chars: 778\n",
      "tokens after removing special chars: 744\n",
      "tokens before removing special chars: 886\n",
      "tokens after removing special chars: 847\n",
      "tokens before removing special chars: 829\n",
      "tokens after removing special chars: 798\n",
      "tokens before removing special chars: 1050\n",
      "tokens after removing special chars: 1004\n",
      "tokens before removing special chars: 928\n",
      "tokens after removing special chars: 894\n",
      "tokens before removing special chars: 924\n",
      "tokens after removing special chars: 893\n",
      "tokens before removing special chars: 1043\n",
      "tokens after removing special chars: 998\n",
      "tokens before removing special chars: 973\n",
      "tokens after removing special chars: 934\n",
      "tokens before removing special chars: 1063\n",
      "tokens after removing special chars: 1031\n",
      "all_issues_tokens after this step: 10106\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Remove extra whitespace\n",
   "id": "c65349282c9ea4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.590216Z",
     "start_time": "2025-07-07T20:54:24.588821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_extra_whitespace(text: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ],
   "id": "e4e16a6edda8471c",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:54:24.602857Z",
     "start_time": "2025-07-07T20:54:24.598559Z"
    }
   },
   "cell_type": "code",
   "source": "all_issues_tokens = process_issues_content(remove_extra_whitespace, \"removing extra whitespace\")",
   "id": "8ef1eed30aaf4e46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_issues_tokens before this step: 10106\n",
      "tokens before removing extra whitespace: 925\n",
      "tokens after removing extra whitespace: 874\n",
      "tokens before removing extra whitespace: 1038\n",
      "tokens after removing extra whitespace: 993\n",
      "tokens before removing extra whitespace: 744\n",
      "tokens after removing extra whitespace: 702\n",
      "tokens before removing extra whitespace: 847\n",
      "tokens after removing extra whitespace: 803\n",
      "tokens before removing extra whitespace: 798\n",
      "tokens after removing extra whitespace: 758\n",
      "tokens before removing extra whitespace: 1004\n",
      "tokens after removing extra whitespace: 956\n",
      "tokens before removing extra whitespace: 894\n",
      "tokens after removing extra whitespace: 855\n",
      "tokens before removing extra whitespace: 893\n",
      "tokens after removing extra whitespace: 848\n",
      "tokens before removing extra whitespace: 998\n",
      "tokens after removing extra whitespace: 951\n",
      "tokens before removing extra whitespace: 934\n",
      "tokens after removing extra whitespace: 892\n",
      "tokens before removing extra whitespace: 1031\n",
      "tokens after removing extra whitespace: 984\n",
      "all_issues_tokens after this step: 9616\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Thanks to all of these techniques, amount of tokens is decreased from 10840 to 9616."
   ],
   "id": "d91b67a4b046c539"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
